% Created 2013-02-27 Wed 00:40
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\providecommand{\alert}[1]{\textbf{#1}}

\title{CS 1573: Term Project Proposal}
\author{Rishi Sadhir}
\date{\today}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs Org-mode version 7.8.11}}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents
\vspace*{1cm}


We, Joel Griffith, Kathleen Marinan, and Rishi Sadhir, propose a project in which we apply machine learning techniques to the problem of identifying and sorting incoming emails into folders based on message content and user preferences.

\section{Problem Introduction}
\label{sec-1}


Effective use of email communication has become an essential skill in modern life, being used in a variety of ways, from personal to corporate.  Different email users have different preferences for sorting their incoming mail.  Some are determined to maintain personal control over choosing folders into which their mail gets put, but as their volume of email increases, being able to determine an appropriate categorization scheme for incoming messages could prove incredibly useful.  However, because people are prone to organizational idiosyncrasies, a one-size-fits-all sorting algorithm would be inappropriate.  Using machine learning to customize the sorting algorithm for each persons individual mailbox makes more sense, and is the problem we will try to solve.
Imagine a person who has been manually sorting their emails into folders. Our goal is to be able to learn this user’s classification technique and be able to take over for him. This problem is particularly interesting because of potentially needing to “understand” the contents of a file in order to compare it to similar files. This also, however, poses a problem of complexity. The number of words in one’s mail archive is astoundingly large, and attempting to understand relationships among them is a very complex problem.
We will begin with a naive bag of words approach where in each word is a feature, with its weight being a function of its frequency of appearance per training example (individual email) and its frequency in a particular class (mail folder). This naive feature set will be expanded to trying to understand various users within the system by analyzing the “to” and “from” field of each email. These name features will be given additional importance by giving them extra weight. Also to be included at this level of analysis would be buckets for time. A time of day feature will be split into dummies such as evening, night, morning, and work-hours. Month will as well be given dummy variables to train on.
At the third level, we will introduce techniques to reduce the noise among the feature set. We hope to aggressively filter around 95\% of the features (words) by using information gain, which measures the number of bits of information obtained for the prediction of categories by knowing the presence or absence in a document of the feature f. The probabilities are computed as ratios of frequencies in the training data. Another approach is to use chi-square which measures the maximal strength of dependence between the feature and the categories. Experiments show that both measures can reduce the dimensionality by a factor of 100 without loss of categorization quality or even with a small improvement.
\section{Related Works}
\label{sec-2}


Our primary benchmark foray into email classification comes from University of Massachusetts researchers Ron Bekkerman, Andrew McCallum, and Gary Huang in their publication “Automatic Categorization of Email into Folders: Benchmark Experiments on Enron and SRI Corpora.”  They took the raw Enron dataset and significantly reduced its size by considering only the top users in terms of number of folders and emails. Once they had a reduced dataset, they did further preprocessing to eliminate per-user folders that contained emails with no clear correlation between them and the folder name. We plan to utilize a similar approach when preprocessing our dataset.
Their input vector consisted of word counts for all known words, ignoring the top 100 most common words and once-used words. Approaches used in their analysis include Support Vector Machine (SVM), as well as a modified version of Winnow referred to as “Wide-Margin” Winnow. The SVM appears to offer the highest accuracy, although “Wide-Margin” Winnow performs comparably.
The authors of the paper ran into a few problems they addressed at the conclusion of their paper.  Namely that, contrary to what was expected, sorting the training data by date did not perform better than a random permutation of the dataset.  A second problem, this time with the algorithms chosen, was that the classifiers tended to perform poorly when a new classification would be introduced.
Another effort from the University of Texas by Chakravarthy et al utilized A Graph Based Approach for Multi-Folder Email Classification.  In their publication, they point out some of the flaws of some of the more common approaches (including SVM and TF-IDF) in which the structural relationships and the inherent hierarchy of information contained in the manner in which an email is constructed are discarded.  This loss of information is analogous to reducing the dimensionality of a more general problem.  Instead of treating the document like a “bag” of words, each email was treated as a graph, thereby preserving some of the structural nature of the email.  They also preprocessed the messages used through stop-word elimination (removing conjunctions, articles, etc), stemming (reducing inflected words to their roots/base/stem), and feature selection (ranking words based on their occurrence frequency across the documents).
Another publication, Email Folder Classification Using Threads, provides a description of three classes of features present in emails: unstructured text, semi-structured text, and numeric data, with an additional suggested class of relationships, e.g. between emails and between users.  It is one of these relationships which is the topic of the paper.  Email threads are considered to be chains that relate a set emails among a set of users discussing a particular topic.

In addition to the overall problem of email classification, a crucial component of the success of our algorithm relies on how our features are selected.  In A Comparative Study on Feature Selection in Text Categorization, Yang and Pederson compared five methods of term selection: document frequency, information gain, mutual information, a Chi-squared test, and term strength.  They concluded that information gain and Chi-square were most effective in term removal without losing accuracy in overall categorization.  Additionally, document frequency was comparable to these two with up to 90\% term removal; this suggests document frequency can be considered a reliable measure for selecting informative features.
\section{Resources Needed}
\label{sec-3}


The data is available to us as a hierarchy of folders representing each user’s email classifications as folders, and their emails within them. It is available here (\href{http://www.cs.cmu.edu/~enron/}{http://www.cs.cmu.edu/\~enron/}). It does not make sense to aggregate each user’s emails into one giant dataset because categorization is usually done at a personal level. Thus, we will examine the five most prolific emailers, and remove any folder categorization with less than thirty emails.
After securing the data, quite a bit of preprocessing will need to be done. Representing all of the features as a numerical vector will be a large task in and of itself. First thing to do is parse every email for names, as we have decided that names deserve extra weight. We must also parse for date, and extract time of day, as well as month from it. The second thing we must do is truncate each of the files, removing words that we have deemed unimportant via the techniques mentioned above (information gain and chi square). Lastly, for each training example (email) we build the feature vector according to TF-IDF of the remaining words, <mailbox$_{\mathrm{name}}$> <TF-IDF W1> … <TF-IDF Wn> <month> <time-of-day bucket> <person1> … <person2>.
\section{Project Setup}
\label{sec-4}
\subsection{Data collection}
\label{sec-4-1}


Common classifiers and learning algorithms cannot directly process the text documents in their original form. Therefore, during a preprocessing step, the documents are converted into a more manageable representation. Typically the documents are represented by feature vectors. The most common bag of words model simply uses all words in a document as the features, and thus the dimension of the feature space is equal to the number of different words in all of the documents.
One of the main difficulties with approaching problems like this is this huge feature space. Analysis of documents requires, in at least the naive base case, a feature for each word. One may go even further and try to analyze relationships among words, in which case we find an exponential number of features to pile on. The simple bag of words feature space for a big collection of documents can reach hundreds of thousands. Most of these words are irrelevant to categorization, and can be dropped with no harm to the classifier performance, and may likely result in an improvement due to a reduction in noise. Deciding on how to shave off these extraneous features presents a problem of its own. As discussed earlier, we will use information gain and chi-square to reduce the feature set.
As for the features themselves, we will use term frequency–inverse document frequency (tfidf) as weights. TF-IDF$_{\mathrm{WEIGHT}}$(w,d) = TermFreq(w,d) * log(N/DocFreq(w)) where w is word and d is doc.
\subsection{Model choice}
\label{sec-4-2}


What algorithm should you use to find the target function?
We anticipate the problem to be linearly-separable, and will assume that our target function will be a linear model. We will experiment with logistic regression, neural networks, and support vector machines, as they appear to be the go-to algorithms for text--and email--categorization.
\subsection{Training}
\label{sec-4-3}


Because we are framing the training of the function on a single user’s dataset, we anticipate the use of k-fold cross validation in creating our training and testing data sets. Previous efforts have “randomized” the data in various ways, usually by the date of the email. We plan on randomizing the data based on the time-of-day feature such that no single time-of-day dominates the training examples.

Information gain and Chi-square could be used to prune the words present in the emails to reduce noise and computational intensity.
\section{Conclusion format}
\label{sec-5}


Evaluation will be rather easy to accomplish. For each user, we will perform k-fold cross validation on each of the selected users emails, and since they are all already classified by their folders, we will know the exact percentage of emails that were mislabeled. With this information we can use paired t tests to compare the algorithms, and preparation techniques used to get the results. All of the results will be tabulated for each user’s inbox and cross compared to determine which ones perform best under which conditions.
\section{Work plan Be specific; discuss who is responsible for which parts; include dates and milestones.}
\label{sec-6}


Following the submission of this proposal, there are just over five weeks (excluding spring break) before the first day of project presentations.  We aim to have all of our data preprocessing completed before spring break, i.e. by March 9.  Following the completion of the data preprocessing, each member of the team will be responsible for implementing one of the learning algorithms (though collaboration will be utilized as needed) as follows: Rishi implementing logistic regression, Kathleen implementing a neural network, and Joel implementing a support vector machine.  We will have approximately two weeks to code and debug each algorithm, finishing by April 1st.  Then, having all of the algorithms complete, we will collaborate on the statistical analysis and comparison of the models.  This should not take more than a week (complete by April 9th), and then we will have the remaining time before the final week of classes to create and prepare our presentation for the class.

\end{document}
